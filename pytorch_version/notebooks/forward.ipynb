{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "366a1bf8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/haoranhuang/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import go\n",
                "arg_list = go.setup_running_env()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "276961cd",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "5bfb905f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "08/09/2022 07:42:02 - INFO - run_ner_softmax:463 -  device_name is cuda\n",
                        "08/09/2022 07:42:02 - WARNING - run_ner_softmax:473 -  Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
                        "08/09/2022 07:42:02 - INFO - run_ner_softmax:485 -  task_name is cluener\n",
                        "08/09/2022 07:42:02 - INFO - run_ner_softmax:495 -  model_type is bert\n",
                        "08/09/2022 07:42:02 - INFO - configuration_utils:120 -  \n",
                        "=============Loading configuration from pretrain. kwargs: {'num_labels': 34, 'loss_type': 'ce', 'cache_dir': '/home/haoranhuang/.cache/torch/transformers'}\n",
                        "08/09/2022 07:42:02 - INFO - configuration_utils:151 -  loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /home/haoranhuang/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.f12a4f986e43d8b328f5b067a641064d67b91597567a06c7b122d1ca7dfd9741\n",
                        "08/09/2022 07:42:02 - INFO - configuration_utils:171 -  Model config {\n",
                        "  \"architectures\": [\n",
                        "    \"BertForMaskedLM\"\n",
                        "  ],\n",
                        "  \"attention_probs_dropout_prob\": 0.1,\n",
                        "  \"directionality\": \"bidi\",\n",
                        "  \"finetuning_task\": null,\n",
                        "  \"hidden_act\": \"gelu\",\n",
                        "  \"hidden_dropout_prob\": 0.1,\n",
                        "  \"hidden_size\": 768,\n",
                        "  \"initializer_range\": 0.02,\n",
                        "  \"intermediate_size\": 3072,\n",
                        "  \"layer_norm_eps\": 1e-12,\n",
                        "  \"loss_type\": \"ce\",\n",
                        "  \"max_position_embeddings\": 512,\n",
                        "  \"model_type\": \"bert\",\n",
                        "  \"num_attention_heads\": 12,\n",
                        "  \"num_hidden_layers\": 12,\n",
                        "  \"num_labels\": 34,\n",
                        "  \"output_attentions\": false,\n",
                        "  \"output_hidden_states\": false,\n",
                        "  \"output_past\": true,\n",
                        "  \"pad_token_id\": 0,\n",
                        "  \"pooler_fc_size\": 768,\n",
                        "  \"pooler_num_attention_heads\": 12,\n",
                        "  \"pooler_num_fc_layers\": 3,\n",
                        "  \"pooler_size_per_head\": 128,\n",
                        "  \"pooler_type\": \"first_token_transform\",\n",
                        "  \"pruned_heads\": {},\n",
                        "  \"torchscript\": false,\n",
                        "  \"type_vocab_size\": 2,\n",
                        "  \"use_bfloat16\": false,\n",
                        "  \"vocab_size\": 21128\n",
                        "}\n",
                        "\n",
                        "08/09/2022 07:42:02 - INFO - tokenization_utils:282 -  \n",
                        "=============Loading tokenizer from pretrain. kwargs: {'do_lower_case': True, 'cache_dir': '/home/haoranhuang/.cache/torch/transformers'}\n",
                        "08/09/2022 07:42:03 - INFO - tokenization_utils:374 -  loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /home/haoranhuang/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
                        "08/09/2022 07:42:03 - INFO - modeling_utils:289 -  \n",
                        "=============Loading model from pretrain. kwargs: {'from_tf': False, 'config': {\n",
                        "  \"architectures\": [\n",
                        "    \"BertForMaskedLM\"\n",
                        "  ],\n",
                        "  \"attention_probs_dropout_prob\": 0.1,\n",
                        "  \"directionality\": \"bidi\",\n",
                        "  \"finetuning_task\": null,\n",
                        "  \"hidden_act\": \"gelu\",\n",
                        "  \"hidden_dropout_prob\": 0.1,\n",
                        "  \"hidden_size\": 768,\n",
                        "  \"initializer_range\": 0.02,\n",
                        "  \"intermediate_size\": 3072,\n",
                        "  \"layer_norm_eps\": 1e-12,\n",
                        "  \"loss_type\": \"ce\",\n",
                        "  \"max_position_embeddings\": 512,\n",
                        "  \"model_type\": \"bert\",\n",
                        "  \"num_attention_heads\": 12,\n",
                        "  \"num_hidden_layers\": 12,\n",
                        "  \"num_labels\": 34,\n",
                        "  \"output_attentions\": false,\n",
                        "  \"output_hidden_states\": false,\n",
                        "  \"output_past\": true,\n",
                        "  \"pad_token_id\": 0,\n",
                        "  \"pooler_fc_size\": 768,\n",
                        "  \"pooler_num_attention_heads\": 12,\n",
                        "  \"pooler_num_fc_layers\": 3,\n",
                        "  \"pooler_size_per_head\": 128,\n",
                        "  \"pooler_type\": \"first_token_transform\",\n",
                        "  \"pruned_heads\": {},\n",
                        "  \"torchscript\": false,\n",
                        "  \"type_vocab_size\": 2,\n",
                        "  \"use_bfloat16\": false,\n",
                        "  \"vocab_size\": 21128\n",
                        "}\n",
                        ", 'cache_dir': '/home/haoranhuang/.cache/torch/transformers'}\n",
                        "08/09/2022 07:42:05 - INFO - modeling_utils:357 -  loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at /home/haoranhuang/.cache/torch/transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
                        "08/09/2022 07:42:06 - INFO - modeling_utils:425 -  Weights of BertSoftmaxForNer not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
                        "08/09/2022 07:42:06 - INFO - modeling_utils:428 -  Weights from pretrained model not used in BertSoftmaxForNer: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
                        "08/09/2022 07:42:06 - INFO - run_ner_softmax:517 -  \n",
                        "========\n",
                        "Training/evaluation parameters Namespace(task_name='cluener', data_dir='/home/haoranhuang/workspace/CLUENER2020/pytorch_version/CLUEdatasets/cluener', model_type='bert', model_name_or_path='bert-base-chinese', output_dir='/home/haoranhuang/workspace/CLUENER2020/pytorch_version/outputs/bert-base-chinese/cluener_output/bert', markup='bios', loss_type='ce', labels='', config_name='', tokenizer_name='', cache_dir='/home/haoranhuang/.cache/torch/transformers', train_max_seq_length=128, eval_max_seq_length=512, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, do_lower_case=True, per_gpu_train_batch_size=24, per_gpu_eval_batch_size=24, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=0, logging_steps=224, save_steps=224, eval_all_checkpoints=False, predict_checkpoints=0, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), id2label={0: 'X', 1: 'B-address', 2: 'B-book', 3: 'B-company', 4: 'B-game', 5: 'B-government', 6: 'B-movie', 7: 'B-name', 8: 'B-organization', 9: 'B-position', 10: 'B-scene', 11: 'I-address', 12: 'I-book', 13: 'I-company', 14: 'I-game', 15: 'I-government', 16: 'I-movie', 17: 'I-name', 18: 'I-organization', 19: 'I-position', 20: 'I-scene', 21: 'S-address', 22: 'S-book', 23: 'S-company', 24: 'S-game', 25: 'S-government', 26: 'S-movie', 27: 'S-name', 28: 'S-organization', 29: 'S-position', 30: 'S-scene', 31: 'O', 32: '[CLS]', 33: '[SEP]'}, label2id={'X': 0, 'B-address': 1, 'B-book': 2, 'B-company': 3, 'B-game': 4, 'B-government': 5, 'B-movie': 6, 'B-name': 7, 'B-organization': 8, 'B-position': 9, 'B-scene': 10, 'I-address': 11, 'I-book': 12, 'I-company': 13, 'I-game': 14, 'I-government': 15, 'I-movie': 16, 'I-name': 17, 'I-organization': 18, 'I-position': 19, 'I-scene': 20, 'S-address': 21, 'S-book': 22, 'S-company': 23, 'S-game': 24, 'S-government': 25, 'S-movie': 26, 'S-name': 27, 'S-organization': 28, 'S-position': 29, 'S-scene': 30, 'O': 31, '[CLS]': 32, '[SEP]': 33})\n",
                        "========\n",
                        "\n",
                        "08/09/2022 07:42:06 - INFO - run_ner_softmax:318 -  Loading features from cached file /home/haoranhuang/workspace/CLUENER2020/pytorch_version/CLUEdatasets/cluener/cached_soft-train_bert-base-chinese_128_cluener\n"
                    ]
                }
            ],
            "source": [
                "predict_type = \"softmax\"\n",
                "runner = go.get_runner(predict_type)\n",
                "args, config, tokenizer, model, model_classes = runner.setup(arg_list=arg_list)\n",
                "config_class, model_class, tokenizer_class = model_classes\n",
                "train_dataset = runner.load_and_cache_examples(args, args.task_name, tokenizer, data_type='train')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "d2b846b6",
            "metadata": {},
            "outputs": [],
            "source": [
                "len(train_dataset)\n",
                "\n",
                "for feat in train_dataset:\n",
                "    if len(feat[0]) != 128:\n",
                "        print(\"feat\", len(feat[0]))\n",
                "        break\n",
                "    if len(feat[1]) != 128:\n",
                "        print(\"feat\", len(feat[0]))\n",
                "        break\n",
                "    if len(feat[2]) != 128:\n",
                "        print(\"feat\", len(feat[0]))\n",
                "        break\n",
                "#     empty tensor\n",
                "#     print(feat[3][0])\n",
                "    if len(feat[4]) != 128:\n",
                "        print(\"feat\", len(feat[0]))\n",
                "        break\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "18d6f3a9",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Namespace(task_name='cluener', data_dir='/home/haoranhuang/workspace/CLUENER2020/pytorch_version/CLUEdatasets/cluener', model_type='bert', model_name_or_path='bert-base-chinese', output_dir='/home/haoranhuang/workspace/CLUENER2020/pytorch_version/outputs/bert-base-chinese/cluener_output/bert', markup='bios', loss_type='ce', labels='', config_name='', tokenizer_name='', cache_dir='/home/haoranhuang/.cache/torch/transformers', train_max_seq_length=128, eval_max_seq_length=512, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, do_lower_case=True, per_gpu_train_batch_size=24, per_gpu_eval_batch_size=24, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=0, logging_steps=224, save_steps=224, eval_all_checkpoints=False, predict_checkpoints=0, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=1, device=device(type='cuda'), id2label={0: 'X', 1: 'B-address', 2: 'B-book', 3: 'B-company', 4: 'B-game', 5: 'B-government', 6: 'B-movie', 7: 'B-name', 8: 'B-organization', 9: 'B-position', 10: 'B-scene', 11: 'I-address', 12: 'I-book', 13: 'I-company', 14: 'I-game', 15: 'I-government', 16: 'I-movie', 17: 'I-name', 18: 'I-organization', 19: 'I-position', 20: 'I-scene', 21: 'S-address', 22: 'S-book', 23: 'S-company', 24: 'S-game', 25: 'S-government', 26: 'S-movie', 27: 'S-name', 28: 'S-organization', 29: 'S-position', 30: 'S-scene', 31: 'O', 32: '[CLS]', 33: '[SEP]'}, label2id={'X': 0, 'B-address': 1, 'B-book': 2, 'B-company': 3, 'B-game': 4, 'B-government': 5, 'B-movie': 6, 'B-name': 7, 'B-organization': 8, 'B-position': 9, 'B-scene': 10, 'I-address': 11, 'I-book': 12, 'I-company': 13, 'I-game': 14, 'I-government': 15, 'I-movie': 16, 'I-name': 17, 'I-organization': 18, 'I-position': 19, 'I-scene': 20, 'S-address': 21, 'S-book': 22, 'S-company': 23, 'S-game': 24, 'S-government': 25, 'S-movie': 26, 'S-name': 27, 'S-organization': 28, 'S-position': 29, 'S-scene': 30, 'O': 31, '[CLS]': 32, '[SEP]': 33})\n"
                    ]
                }
            ],
            "source": [
                "print(args)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "c3eb0347",
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
                "from processors.ner_seq import batch_collate_fn\n",
                "\n",
                "\n",
                "# args, train_dataset, model, tokenizer\n",
                "train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
                "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.per_gpu_train_batch_size,\n",
                "                              collate_fn=batch_collate_fn)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "07c91706",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "device(type='cuda')"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "torch.device(\"cuda\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "936c2fd6",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "id": "3602efe9",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "import tools.common as common\n",
                "\n",
                "cpu = common.get_device(\"cpu\")\n",
                "mps = common.get_device(\"mps\")\n",
                "cuda = common.get_device(\"cuda\")\n",
                "target_device = cuda\n",
                "\n",
                "import time\n",
                "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=1,\n",
                "                              collate_fn=batch_collate_fn)\n",
                "\n",
                "\n",
                "# convert batch to input list\n",
                "inputs_list = []\n",
                "for step, batch in enumerate(train_dataloader):\n",
                "#     print(\"======\")\n",
                "#     print(f\"batch is\\n {batch}\")\n",
                "#     print(f\"len(batch) is\\n {len(batch)}\")\n",
                "#     print(f\"batch.shape is\\n {batch[0].shape}\")\n",
                "#     print(batch[0].shape)\n",
                "#     print(batch[1].shape)\n",
                "#     print(batch[2].shape)\n",
                "#     print(batch[3].shape)\n",
                "#     print(batch[4].shape)\n",
                "    \n",
                "    inputs = {\n",
                "        \"input_ids\": batch[0].to(target_device),\n",
                "        \"attention_mask\": batch[1].to(target_device),\n",
                "        \"labels\": batch[3].to(target_device),\n",
                "    }\n",
                "    if args.model_type != \"distilbert\":\n",
                "        # XLM and RoBERTa don\"t use segment_ids\n",
                "        inputs[\"token_type_ids\"] = (batch[2].to(target_device) if args.model_type in [\"bert\", \"xlnet\"] else None)\n",
                "    inputs_list.append(inputs)\n",
                "    break\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "id": "9a5763c7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{0: 'X', 1: 'B-address', 2: 'B-book', 3: 'B-company', 4: 'B-game', 5: 'B-government', 6: 'B-movie', 7: 'B-name', 8: 'B-organization', 9: 'B-position', 10: 'B-scene', 11: 'I-address', 12: 'I-book', 13: 'I-company', 14: 'I-game', 15: 'I-government', 16: 'I-movie', 17: 'I-name', 18: 'I-organization', 19: 'I-position', 20: 'I-scene', 21: 'S-address', 22: 'S-book', 23: 'S-company', 24: 'S-game', 25: 'S-government', 26: 'S-movie', 27: 'S-name', 28: 'S-organization', 29: 'S-position', 30: 'S-scene', 31: 'O', 32: '[CLS]', 33: '[SEP]'}\n"
                    ]
                }
            ],
            "source": [
                "from processors.ner_seq import ner_processors as processors\n",
                "processor = processors[\"cluener\"]()\n",
                "label_map = processor.get_labels()\n",
                "id2label = {i:label for i, label in enumerate(label_map)}\n",
                "print(id2label)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "id": "87afca8d",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "sequence_output is torch.Size([1, 41, 768])\n",
                        "pooled_sequence_output is torch.Size([1, 768])\n",
                        "logits is torch.Size([1, 41, 34])\n",
                        "tensor([ 0.4948, -0.3017, -0.8894, -0.0891,  0.3375,  0.1482,  0.2054,  0.6662,\n",
                        "        -0.4130,  0.1322, -0.3798, -0.2816,  0.0759,  0.3360,  0.2200, -0.0563,\n",
                        "         0.3037, -0.2275,  0.4163, -0.3351, -0.1387,  0.3301,  0.1531,  0.4575,\n",
                        "         0.3724, -0.0987,  0.0260, -0.5006, -0.2478, -0.3950,  0.4386,  0.3392,\n",
                        "        -0.1641,  0.0565], device='cuda:0', grad_fn=<SelectBackward0>)\n"
                    ]
                }
            ],
            "source": [
                "model.zero_grad()\n",
                "model.train()\n",
                "def myforward(self, input_ids, attention_mask=None, token_type_ids=None,\n",
                "            position_ids=None, head_mask=None, labels=None):\n",
                "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
                "    sequence_output = outputs[0]\n",
                "    pooled_sequence_output = outputs[1]\n",
                "    # sequence_output is the hidden states of all tokens (all = length of sentence)\n",
                "    # For example: torch.Size([1, 51, 768])\n",
                "    # pooled_sequence_output the hidden states of the first token\n",
                "    # pooled_sequence_output is torch.Size([1, 768])\n",
                "    print(f\"sequence_output is {sequence_output.shape}\")\n",
                "    print(f\"pooled_sequence_output is {pooled_sequence_output.shape}\")\n",
                "    \n",
                "    # apply drop out in sequence output\n",
                "    sequence_output = self.dropout(sequence_output)\n",
                "    logits = self.classifier(sequence_output)\n",
                "    print(f\"logits is {logits.shape}\")\n",
                "    # print the softmax of first token's prediction\n",
                "    print(logits[0][0])\n",
                "    \n",
                "\n",
                "# print(model.bert)\n",
                "outputs = myforward(model, **inputs_list[0])\n",
                "# print()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9175122e",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}