{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "766451f4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/haoranhuang/opt/anaconda3/envs/ml/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import json\n",
                "import os\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "87669d89",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "/Users/haoranhuang/Documents/GitHub/CLUENER2020/pytorch_version/CLUEdatasets/cluener/cached_soft-train_bert-base-chinese_128_cluener\n",
                        "text length is  50\n",
                        "feat.input_ids lenght is  128  (including zeros)\n",
                        "feat.input_ids lenght is  52\n",
                        "feat.input_mask lenght is  128  (including zeros)\n",
                        "feat.input_mask lenght is  52\n",
                        "feat.label_ids lenght is  128  (including zeros)\n",
                        "feat.label_ids lenght is  52\n",
                        "feat.segment_ids lenght is  128  (including zeros)\n",
                        "feat.segment_ids lenght is  0\n",
                        "52\n",
                        "{'input_ids': [101, 3851, 1555, 7213, 6121, 821, 689, 928, 6587, 6956, 1383, 5439, 3424, 1300, 1894, 1156, 794, 1369, 671, 702, 6235, 2428, 2190, 758, 6887, 7305, 3546, 6822, 6121, 749, 6237, 6438, 511, 1383, 5439, 3424, 6371, 711, 8024, 2190, 4680, 1184, 1744, 1079, 1555, 689, 7213, 6121, 5445, 6241, 8024, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'segment_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label_ids': [31, 3, 13, 13, 13, 31, 31, 31, 31, 31, 7, 17, 17, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_len': 52}\n"
                    ]
                }
            ],
            "source": [
                "base_dir = \"/Users/haoranhuang/Documents/GitHub/CLUENER2020/pytorch_version\"\n",
                "\n",
                "cached_features_file = os.path.join(base_dir, \"CLUEdatasets/cluener/cached_soft-train_bert-base-chinese_128_cluener\")\n",
                "print(cached_features_file)\n",
                "\n",
                "train_file = os.path.join(base_dir, \"CLUEdatasets/cluener/train.json\")\n",
                "json_file = open(train_file)\n",
                "json_lines = json_file.readlines()\n",
                "\n",
                "first_line_json = json.loads(json_lines[0])\n",
                "# Length is 50, adding label 101 and 102 adds up to 52\n",
                "print(f\"text length is \", len(first_line_json[\"text\"]))\n",
                "features = torch.load(cached_features_file)\n",
                "\n",
                "feat = features[0]\n",
                "# 52, and the rest of zeros are padidngs\n",
                "print(\"feat.input_ids lenght is \", len(feat.input_ids), \" (including zeros)\")\n",
                "print(\"feat.input_ids lenght is \", len(list(filter(lambda x: x != 0, feat.input_ids))))\n",
                "print(\"feat.input_mask lenght is \", len(feat.input_mask), \" (including zeros)\")\n",
                "print(\"feat.input_mask lenght is \", len(list(filter(lambda x: x != 0, feat.input_mask))))\n",
                "print(\"feat.label_ids lenght is \", len(feat.label_ids), \" (including zeros)\")\n",
                "print(\"feat.label_ids lenght is \", len(list(filter(lambda x: x != 0, feat.label_ids))))\n",
                "\n",
                "# segment IDs are all zeros as there's only one sentence here.\n",
                "print(\"feat.segment_ids lenght is \", len(feat.segment_ids), \" (including zeros)\")\n",
                "print(\"feat.segment_ids lenght is \", len(list(filter(lambda x: x != 0, feat.segment_ids))))\n",
                "print(feat.input_len)\n",
                "print(feat.to_dict())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "a0e9c3aa",
            "metadata": {},
            "outputs": [],
            "source": [
                "from processors.ner_seq import ner_processors as processors\n",
                "taskName = \"cluener\"\n",
                "processor = processors[taskName]()\n",
                "data_dir = os.path.join(base_dir, \"CLUEdatasets/my_own_datasets/\")\n",
                "os.makedirs(data_dir, exist_ok=True)\n",
                "train_examples = processor.get_train_examples(data_dir)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "d91017c1",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "128"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "all_ids_list = [f.input_ids for f in features]\n",
                "len(all_ids_list[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "31388916",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "08/03/2022 18:49:50 - WARNING - run_ner_softmax:456 -  Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
                        "08/03/2022 18:49:51 - INFO - configuration_utils:150 -  loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /Users/haoranhuang/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.f12a4f986e43d8b328f5b067a641064d67b91597567a06c7b122d1ca7dfd9741\n",
                        "08/03/2022 18:49:51 - INFO - configuration_utils:170 -  Model config {\n",
                        "  \"architectures\": [\n",
                        "    \"BertForMaskedLM\"\n",
                        "  ],\n",
                        "  \"attention_probs_dropout_prob\": 0.1,\n",
                        "  \"directionality\": \"bidi\",\n",
                        "  \"finetuning_task\": null,\n",
                        "  \"hidden_act\": \"gelu\",\n",
                        "  \"hidden_dropout_prob\": 0.1,\n",
                        "  \"hidden_size\": 768,\n",
                        "  \"initializer_range\": 0.02,\n",
                        "  \"intermediate_size\": 3072,\n",
                        "  \"layer_norm_eps\": 1e-12,\n",
                        "  \"loss_type\": \"ce\",\n",
                        "  \"max_position_embeddings\": 512,\n",
                        "  \"model_type\": \"bert\",\n",
                        "  \"num_attention_heads\": 12,\n",
                        "  \"num_hidden_layers\": 12,\n",
                        "  \"num_labels\": 34,\n",
                        "  \"output_attentions\": false,\n",
                        "  \"output_hidden_states\": false,\n",
                        "  \"output_past\": true,\n",
                        "  \"pad_token_id\": 0,\n",
                        "  \"pooler_fc_size\": 768,\n",
                        "  \"pooler_num_attention_heads\": 12,\n",
                        "  \"pooler_num_fc_layers\": 3,\n",
                        "  \"pooler_size_per_head\": 128,\n",
                        "  \"pooler_type\": \"first_token_transform\",\n",
                        "  \"pruned_heads\": {},\n",
                        "  \"torchscript\": false,\n",
                        "  \"type_vocab_size\": 2,\n",
                        "  \"use_bfloat16\": false,\n",
                        "  \"vocab_size\": 21128\n",
                        "}\n",
                        "\n",
                        "08/03/2022 18:49:52 - INFO - tokenization_utils:373 -  loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /Users/haoranhuang/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
                        "08/03/2022 18:49:54 - INFO - modeling_utils:352 -  loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin from cache at /Users/haoranhuang/.cache/torch/transformers/b1b5e295889f2d0979ede9a78ad9cb5dc6a0e25ab7f9417b315f0a2c22f4683d.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
                        "08/03/2022 18:49:57 - INFO - modeling_utils:420 -  Weights of BertSoftmaxForNer not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
                        "08/03/2022 18:49:57 - INFO - modeling_utils:423 -  Weights from pretrained model not used in BertSoftmaxForNer: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
                        "08/03/2022 18:49:57 - INFO - run_ner_softmax:488 -  Training/evaluation parameters Namespace(task_name='cluener', data_dir='/Users/haoranhuang/Documents/GitHub/CLUENER2020/pytorch_version/CLUEdatasets/cluener', model_type='bert', model_name_or_path='bert-base-chinese', output_dir='/Users/haoranhuang/Documents/GitHub/CLUENER2020/pytorch_version/outputs/bert-base-chinese/cluener_output/bert', markup='bios', loss_type='ce', labels='', config_name='', tokenizer_name='', cache_dir='/Users/haoranhuang/.cache/torch/transformers', train_max_seq_length=128, eval_max_seq_length=512, do_train=True, do_eval=True, do_predict=True, evaluate_during_training=False, do_lower_case=True, per_gpu_train_batch_size=24, per_gpu_eval_batch_size=24, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=4.0, max_steps=-1, warmup_steps=0, logging_steps=224, save_steps=224, eval_all_checkpoints=False, predict_checkpoints=0, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, server_ip='', server_port='', n_gpu=0, device=device(type='cpu'), id2label={0: 'X', 1: 'B-address', 2: 'B-book', 3: 'B-company', 4: 'B-game', 5: 'B-government', 6: 'B-movie', 7: 'B-name', 8: 'B-organization', 9: 'B-position', 10: 'B-scene', 11: 'I-address', 12: 'I-book', 13: 'I-company', 14: 'I-game', 15: 'I-government', 16: 'I-movie', 17: 'I-name', 18: 'I-organization', 19: 'I-position', 20: 'I-scene', 21: 'S-address', 22: 'S-book', 23: 'S-company', 24: 'S-game', 25: 'S-government', 26: 'S-movie', 27: 'S-name', 28: 'S-organization', 29: 'S-position', 30: 'S-scene', 31: 'O', 32: '[START]', 33: '[END]'}, label2id={'X': 0, 'B-address': 1, 'B-book': 2, 'B-company': 3, 'B-game': 4, 'B-government': 5, 'B-movie': 6, 'B-name': 7, 'B-organization': 8, 'B-position': 9, 'B-scene': 10, 'I-address': 11, 'I-book': 12, 'I-company': 13, 'I-game': 14, 'I-government': 15, 'I-movie': 16, 'I-name': 17, 'I-organization': 18, 'I-position': 19, 'I-scene': 20, 'S-address': 21, 'S-book': 22, 'S-company': 23, 'S-game': 24, 'S-government': 25, 'S-movie': 26, 'S-name': 27, 'S-organization': 28, 'S-position': 29, 'S-scene': 30, 'O': 31, '[START]': 32, '[END]': 33})\n",
                        "08/03/2022 18:49:57 - INFO - run_ner_softmax:317 -  Creating features from dataset file at /Users/haoranhuang/Documents/GitHub/CLUENER2020/pytorch_version/CLUEdatasets/cluener\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:82 -  Writing example 0 of 10748\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:145 -  *** Example ***\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:146 -  guid: train-0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:147 -  tokens: [CLS] 浙 商 银 行 企 业 信 贷 部 叶 老 桂 博 士 则 从 另 一 个 角 度 对 五 道 门 槛 进 行 了 解 读 。 叶 老 桂 认 为 ， 对 目 前 国 内 商 业 银 行 而 言 ， [SEP]\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:148 -  input_ids: 101 3851 1555 7213 6121 821 689 928 6587 6956 1383 5439 3424 1300 1894 1156 794 1369 671 702 6235 2428 2190 758 6887 7305 3546 6822 6121 749 6237 6438 511 1383 5439 3424 6371 711 8024 2190 4680 1184 1744 1079 1555 689 7213 6121 5445 6241 8024 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:149 -  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:150 -  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:151 -  label_ids: 31 3 13 13 13 31 31 31 31 31 7 17 17 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:145 -  *** Example ***\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:146 -  guid: train-1\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:147 -  tokens: [CLS] 生 生 不 息 c s o l 生 化 狂 潮 让 你 填 弹 狂 扫 [SEP]\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:148 -  input_ids: 101 4495 4495 679 2622 145 161 157 154 4495 1265 4312 4060 6375 872 1856 2486 4312 2812 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:149 -  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:150 -  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:151 -  label_ids: 31 31 31 31 31 4 14 14 14 31 31 31 31 31 31 31 31 31 31 31 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "08/03/2022 18:49:57 - INFO - ner_seq:145 -  *** Example ***\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:146 -  guid: train-2\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:147 -  tokens: [CLS] 那 不 勒 斯 v s 锡 耶 纳 以 及 桑 普 v s 热 那 亚 之 上 呢 ？ [SEP]\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:148 -  input_ids: 101 6929 679 1239 3172 164 161 7234 5456 5287 809 1350 3433 3249 164 161 4178 6929 762 722 677 1450 8043 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:149 -  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:150 -  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:151 -  label_ids: 31 8 18 18 18 31 31 8 18 18 31 31 8 18 31 31 8 18 18 31 31 31 31 31 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:145 -  *** Example ***\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:146 -  guid: train-3\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:147 -  tokens: [CLS] 加 勒 比 海 盗 3 ： 世 界 尽 头 》 的 去 年 同 期 成 绩 死 死 甩 在 身 后 ， 后 者 则 即 将 赶 超 《 变 形 金 刚 》 ， [SEP]\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:148 -  input_ids: 101 1217 1239 3683 3862 4668 124 8038 686 4518 2226 1928 518 4638 1343 2399 1398 3309 2768 5327 3647 3647 4501 1762 6716 1400 8024 1400 5442 1156 1315 2199 6628 6631 517 1359 2501 7032 1157 518 8024 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:149 -  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:150 -  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:151 -  label_ids: 31 6 16 16 16 16 16 16 16 16 16 16 16 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 31 6 16 16 16 16 16 31 31 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:145 -  *** Example ***\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:146 -  guid: train-4\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:147 -  tokens: [CLS] 布 鲁 京 斯 研 究 所 桑 顿 中 国 中 心 研 究 部 主 任 李 成 说 ， 东 亚 的 和 平 与 安 全 ， 是 美 国 的 [UNK] 核 心 利 益 [UNK] 之 一 。 [SEP]\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:148 -  input_ids: 101 2357 7826 776 3172 4777 4955 2792 3433 7561 704 1744 704 2552 4777 4955 6956 712 818 3330 2768 6432 8024 691 762 4638 1469 2398 680 2128 1059 8024 3221 5401 1744 4638 100 3417 2552 1164 4660 100 722 671 511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:149 -  input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:150 -  segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:57 - INFO - ner_seq:151 -  label_ids: 31 8 18 18 18 18 18 18 18 18 18 18 18 18 9 19 19 19 19 7 17 31 31 31 31 31 31 31 31 31 31 31 31 1 11 31 31 31 31 31 31 31 31 31 31 31 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
                        "08/03/2022 18:49:58 - INFO - ner_seq:82 -  Writing example 10000 of 10748\n",
                        "08/03/2022 18:49:58 - INFO - run_ner_softmax:340 -  Saving features into cached file /Users/haoranhuang/Documents/GitHub/CLUENER2020/pytorch_version/CLUEdatasets/cluener/cached_soft-train_bert-base-chinese_128_cluener\n"
                    ]
                }
            ],
            "source": [
                "# Get converted dataset (features)\n",
                "import go\n",
                "predict_type = \"softmax\"\n",
                "arg_list = go.get_args()\n",
                "runner = go.get_runner(predict_type)\n",
                "args, config, tokenizer, model, model_classes = runner.setup(arg_list=arg_list)\n",
                "train_dataset = runner.load_and_cache_examples(args, args.task_name, tokenizer, data_type='train')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "e9960f29",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(tensor([ 101, 4495, 4495,  679, 2622,  145,  161,  157,  154, 4495, 1265, 4312,\n",
                            "         4060, 6375,  872, 1856, 2486, 4312, 2812,  102,    0,    0,    0,    0,\n",
                            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
                            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
                            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
                            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
                            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
                            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
                            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
                            "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
                            "            0,    0,    0,    0,    0,    0,    0,    0]),\n",
                            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
                            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                            "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
                            " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
                            "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
                            " tensor(20),\n",
                            " tensor([31, 31, 31, 31, 31,  4, 14, 14, 14, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
                            "         31, 31,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
                            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
                            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
                            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
                            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
                            "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
                            "          0,  0]))"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# TensorDataset - basically a stack of tensors\n",
                "type(train_dataset)\n",
                "#\n",
                "# all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
                "# all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
                "# all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
                "# all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
                "# all_lens = torch.tensor([f.input_len for f in features], dtype=torch.long)\n",
                "# dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_lens, all_label_ids)\n",
                "train_dataset[0]"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}